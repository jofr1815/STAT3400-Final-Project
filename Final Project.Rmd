---
title: "Final Project"
author: "John Frederickson"
date: "4/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analysis of Non-Voter Data

## Introduction

For my final project, I will be doing an analysis of Non-Voter data, from fivethirtyeight's non-voter data set. I will first perform an analysis of the basic variables in order to draw preliminary insight as which influence someones likelihood to vote. I will then split the model into two sets, a training set (80%) and a testing set (20%), and create a preliminary prediction model. I will then perform a more in depth analysis with more than the basic variables. This will also include creating artificial scores from the questions with multiple parts, such as trust in different parts of the government being added together for a generic "trust in government" score. A larger prediction model will be fitted, and tested similar to the first model. Time permitting, and depending on what ends up being significant in the larger model, these artificial scores may be split up to determine which factors exactly are influential on voter category. 

### Background Information
//backround info from proposal

### About the Data Set

Data Set Link: https://github.com/fivethirtyeight/data/tree/master/non-voters

This data set includes survey results from 5,239 participants, who were surveyed between September 15 2020 and September 25 2020. Initially 8,327 participants were surveyed, though only 64% (5,239) of the results were included based on those who were eligible to vote for at three or more elections, matching the U.S. Census Bureau's population benchmarks (to avoid over or under representing certain demographics), and eliminating most respondents whose information given did not match the voter file. Some of those not found in the voter file were left in the results, to avoid under representing non-voters who are eligible and just not registered.

Basic information was collected such as income, gender, education, weight, race, and age as well as how often they vote. Each participants were put into one of three categories, never or rarely vote (voted in 0 or 1 elections), sometimes vote (more than 1, less than all but 1), and nearly always vote (voted in all or all but one election), based on how often they have voted in elections they are eligible for. Each respondent was also asked a variety of other questions, such as their political affiliation, their preferred method of voting, their plans for the 2020 election, whether their vote matters, their trust in government, barriers to voting, and whether or not the government needs change. Many of these questions were split up into sub questions, such as trust in specific parts of the government, and have multiple levels for each (A lot, some, not much, not at all, etc). 

### Initial Setup

```{r}
# Load data set
vote <- read.csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv")

# Load libraries
library(dplyr)
library(ggplot2)
library(cowplot)
```

#### Training and testing set

```{r}
# Generate a random vector to label which rows will be split
library(caTools)
split <- sample.split(vote$RespId, SplitRatio = 0.8)
summary(split)
```

#### Recoding variables

First, we transform the voter categories to a numeric variable and assign it to a new column
Levels: always -> 3
        sporadic -> 2
        never -> 1
This will be useful later to examine and compare means of levels of different variables

```{r}
# Transform voter categories from character levels to a numerica variable
# -Useful for calculating means (anova/bartlett, building mlr model

vote$voteChance <- as.numeric(recode_factor(vote$voter_category, "rarely/never" = 1, "sporadic" = 2, "always" = 3))

head(vote$voteChance)
summary(vote$voteChance)
head(vote)
```



## Part 1: Basic Variables

For now, lets just explore the basic variables, such as income, education, age, gender, and voter category . This will build a foundation for the techniques to be used in the full scale analysis. 

### 1a: Basic Analysis

#### Visualizations of Voter Category Distributions
//Maybe just have the population distribution plots here, and move voter category to the percentage based plots?

//Change to percentage plot rather than count?

```{r}
ggplot(vote, aes(x = voter_category, fill = voter_category)) + geom_bar()
```


```{r fig1, fig.height=10, fig.width=12}
library(ggplot2)
library(cowplot)
library(dplyr)

# Remove just the basic variables
voteBasic <- vote %>% select(educ, race, gender, income_cat, ppage, voter_category, weight, voteChance)

head(voteBasic)

#ToDo: 1. Plots currently show count in each bin, but need to show percentage
#         -Maybe a different cluster of plots, so that we can visualize the distribution of the population     across each bin?
#      2. Transform weight and age to discrete (leveled) variables?
#         -Useful for visualization, but introduces additional error in analysis
#      3. Remove all legends except one
#
#format: basicBar1 -> basic bar plot #11

#Education
basicBar1 <- ggplot(voteBasic, aes(x = educ, fill = voter_category)) + # Create basic Plot
  geom_bar() + 
  xlab("Education") + ylab("# of Respondents") + # Add axis labels
  guides(fill = guide_legend(title = "Voter Category")) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Rotate category labels 

#Income
basicBar2 <- ggplot(voteBasic, aes(x = income_cat, fill = voter_category)) + # Create basic Plot
  geom_bar() + 
  xlab("Income") + ylab("# of Respondents") + # Add axis labels
  guides(fill = guide_legend(title = "Voter Category")) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Rotate category labels

#Age
#Note: Age is continuous, use histogram rather than bar plot
basicBar3 <- ggplot(voteBasic, aes(x = ppage, fill = voter_category)) + # Create basic Plot
  geom_histogram(breaks = c(20, 30, 40, 50, 60, 70, 80, 90)) + 
  xlab("Age") + ylab("# of Respondents") + # Add axis labels
  guides(fill = guide_legend(title = "Voter Category")) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Rotate category labels

#Gender
basicBar4 <- ggplot(voteBasic, aes(x = gender, fill = voter_category)) + # Create basic Plot
  geom_bar() + 
  xlab("Gender") + ylab("# of Respondents") + # Add axis labels
  guides(fill = guide_legend(title = "Voter Category")) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Rotate category labels

#Race
basicBar5 <- ggplot(voteBasic, aes(x = race, fill = voter_category)) + # Create basic Plot
  geom_bar() + 
  xlab("Race") + ylab("# of Respondents") + # Add axis labels
  guides(fill = guide_legend(title = "Voter Category")) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Rotate category labels

#Weight
#Note: Continuous variable, use histogram rather than bar plot
basicBar6 <- ggplot(voteBasic, aes(x = weight, fill = voter_category)) + # Create basic Plot
  geom_histogram() + 
  xlab("Weight") + ylab("# of Respondents") + # Add axis labels
  guides(fill = guide_legend(title = "Voter Category"))

plot_grid(basicBar1, basicBar2, basicBar3, basicBar4, basicBar5, basicBar6)

```

These plots are interesting and give some insight as to how the sample is distributed among the basic variables. The varying heights (coming from a count function, rather than a percentage) make it difficult to discern which levels have differing distributions of voter category.

//Observations here

Lets transform those to be percentage based: 

```{r}
#Percentage based plots here
```

// Observations here
These transformed plots show the distribution of voter category as a percentage, rather than as a count of the participants in each category. This allows us to better compare the different distributions of voter category across different levels of a variable.

// Next steps for basic analysis:
  1. Box Plots?
  2. Anova to determine which levels are significant (use TukeyHSD to simplify?)
  3. Bartlett test to determine if variances are the same among the different levels
  
Next, lets create a set of box plots to visualize the means and variance of each level. In addition it will also help identify any outliers

```{r}
# Box plots here
```

//Observations

Now, lets use anova to determine which of these levels is significant. 
//anova explanation, explain use of tukeyHSD to simplify finding which specific levels

```{r}
educResult <- aov(voteChance ~ educ, data = voteBasic)
summary(educResult)
```
With a P-value less than 0.05, we can conclude that there is some meaningful difference in mean of voter category by different levels of education. Now lets use TukeyHSD to find which specific levels are significant:

```{r}
TukeyHSD(educResult)
```

//More or less do this for all of the basic prediction variables, note results, and compare to box plots

```{r}
incomeResult <- aov(voteChance ~ income_cat, data = voteBasic)
summary(incomeResult)
TukeyHSD(incomeResult)
```

```{r}
genderResult <- aov(voteChance ~ gender, data = voteBasic)
summary(genderResult)
TukeyHSD(genderResult)
```

```{r}
raceResult <- aov(voteChance ~ race, data = voteBasic)
summary(raceResult)
TukeyHSD(raceResult)
```



### 1b: Basic Prediction Model

First and foremost, we need to use the split vector generated in the initial setup to assign rows to either the training or testing set. 
```{r}
basicTraining <- subset(voteBasic, split == TRUE)
basicTesting  <- subset(voteBasic, split == FALSE)
```

#### The Full Model

Now we can create the initial (full) model from the training set.

```{r}
#lmodb1 -> basic linear model #1

lmodb1 <- lm(voteChance ~ income_cat + ppage + weight + educ + gender + race, data = basicTraining)
summary(lmodb1)
```
//Problems: 1. Due to the nature of voter category being a factor variable, the residuals of this model are likely to be extremely large, and it may not be very accurate.

// Solutions:
            1. Refactor predicted values to the initial three categories, and analyze how often the model places a voter into the correct category?
              - ex: 
              
                  $\hat y < 1.5$ -> 1 
                  
                  $\hat y >= 1.5 & \hat y < 2.5 $ -> 2
                  
                  $\hat y >=2.5$ -> 3
                  
              -Metrics for accuracy?
              -Implications for error?
              -Different penalties for being one group off vs two groups off?
            2. Some other sort of regression technique/function?
            
//In short, this model is going to be quite a mess. Hopefully it can lead to some insight, but most likely isn't going to be super useful. Initially I had it in my head that this model would predict someones likelyhood to vote, but in fact the model can really only predict which group a participant belongs in, as this is the only data we have about a participants voting frequency. 

#### Diagnostics and Model Selection
//Diagnostics here

// Model selection here
1. Backwards Elimination
2. Adjusted R-Squared
3. AIC/BIC?


Since we are attempting to build a prediction model, we will try to find the model with the smallest mean squared error (MSE)

//Compare reduced model to full model here

#### Testing the Basic Model




### 1c: Initial Conclusions

// Initial conclusions here:
//    1. Initial visualizations
//    2. Anova (which variables and levels have a significant effect on voter category)
//    3. Model
//    4. Implications for the real world (how to increase voter turnout by identifying barriers of entry?)
//    4. Next steps for full dataset

## Part 2: The Full Dataset

Part 2 of this analysis will use the methods of part 1, but applied to a larger set of variables. Hopefully this will lead to insight as to a voter's attitude towards the election and/or government, experiencing barriers to voting, party affiliation, and how this influences their likelyhood to vote. Part of this will also include the creation of composite scores (based on questions will multiple answers), as there are many questions with 5+ sub questions. 

### 2a: Analysis


### 2b: Prediction Model

### 2c: Conclusions


## Final conclusion

